{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Sarsa 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구현에 사용할 패키지 임포트하기\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 초기 상태의 미로 모습\n",
    "\n",
    "# 전체 그림의 크기 및 그림을 나타내는 변수 선언\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "# 붉은 벽 그리기\n",
    "plt.plot([1, 1], [0, 1], color='red', linewidth=2)\n",
    "plt.plot([1, 2], [2, 2], color='red', linewidth=2)\n",
    "plt.plot([2, 2], [2, 1], color='red', linewidth=2)\n",
    "plt.plot([2, 3], [1, 1], color='red', linewidth=2)\n",
    "\n",
    "# 상태를 의미하는 문자열(S0~S8) 표시\n",
    "plt.text(0.5, 2.5, 'S0', size=14, ha='center')\n",
    "plt.text(1.5, 2.5, 'S1', size=14, ha='center')\n",
    "plt.text(2.5, 2.5, 'S2', size=14, ha='center')\n",
    "plt.text(0.5, 1.5, 'S3', size=14, ha='center')\n",
    "plt.text(1.5, 1.5, 'S4', size=14, ha='center')\n",
    "plt.text(2.5, 1.5, 'S5', size=14, ha='center')\n",
    "plt.text(0.5, 0.5, 'S6', size=14, ha='center')\n",
    "plt.text(1.5, 0.5, 'S7', size=14, ha='center')\n",
    "plt.text(2.5, 0.5, 'S8', size=14, ha='center')\n",
    "plt.text(0.5, 2.3, 'START', ha='center')\n",
    "plt.text(2.5, 0.3, 'GOAL', ha='center')\n",
    "\n",
    "# 그림을 그릴 범위 및 눈금 제거 설정\n",
    "ax.set_xlim(0, 3)\n",
    "ax.set_ylim(0, 3)\n",
    "plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
    "                labelbottom=False, right=False, left=False, labelleft=False)\n",
    "\n",
    "# S0에 녹색 원으로 현재 위치를 표시\n",
    "line, = ax.plot([0.5], [2.5], marker=\"o\", color='g', markersize=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정책을 결정하는 파라미터의 초깃값 theta_0를 설정\n",
    "\n",
    "# 줄은 상태 0~7, 열은 행동방향(상,우,하,좌 순)를 나타낸다\n",
    "theta_0 = np.array([[np.nan, 1, 1, np.nan],  # s0\n",
    "                    [np.nan, 1, np.nan, 1],  # s1\n",
    "                    [np.nan, np.nan, 1, 1],  # s2\n",
    "                    [1, 1, 1, np.nan],  # s3\n",
    "                    [np.nan, np.nan, 1, 1],  # s4\n",
    "                    [1, np.nan, np.nan, np.nan],  # s5\n",
    "                    [1, np.nan, np.nan, np.nan],  # s6\n",
    "                    [1, 1, np.nan, np.nan],  # s7、※s8은 목표지점이므로 정책이 없다\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정책 파라미터 theta_0을 무작위 행동 정책 pi로 변환하는 함수\n",
    "\n",
    "def simple_convert_into_pi_from_theta(theta):\n",
    "    '''단순 비율 계산'''\n",
    "\n",
    "    [m, n] = theta.shape  # theta의 행렬 크기를 구함\n",
    "    pi = np.zeros((m, n))\n",
    "    for i in range(0, m):\n",
    "        pi[i, :] = theta[i, :] / np.nansum(theta[i, :])  # 비율 계산\n",
    "\n",
    "    pi = np.nan_to_num(pi)  # nan을 0으로 변환\n",
    "\n",
    "    return pi\n",
    "\n",
    "# 무작위 행동정책 pi_0을 계산\n",
    "pi_0 = simple_convert_into_pi_from_theta(theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동가치 함수 Q의 초기 상태\n",
    "\n",
    "[a, b] = theta_0.shape  # 열과 행의 갯수를 변수 a, b에 저장\n",
    "Q = np.random.rand(a, b) * theta_0\n",
    "# * theta0 로 요소 단위 곱셈을 수행, Q에서 벽 방향으로 이동하는 행동에는 nan을 부여\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ε-greedy 알고리즘 구현\n",
    "\n",
    "\n",
    "def get_action(s, Q, epsilon, pi_0):\n",
    "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
    "\n",
    "    # 행동을 결정\n",
    "    if np.random.rand() < epsilon:\n",
    "        # 확률 ε로 무작위 행동을 선택함\n",
    "        next_direction = np.random.choice(direction, p=pi_0[s, :])\n",
    "    else:\n",
    "        # Q값이 최대가 되는 행동을 선택함\n",
    "        next_direction = direction[np.nanargmax(Q[s, :])]\n",
    "\n",
    "    # 행동을 인덱스로 변환\n",
    "    if next_direction == \"up\":\n",
    "        action = 0\n",
    "    elif next_direction == \"right\":\n",
    "        action = 1\n",
    "    elif next_direction == \"down\":\n",
    "        action = 2\n",
    "    elif next_direction == \"left\":\n",
    "        action = 3\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def get_s_next(s, a, Q, epsilon, pi_0):\n",
    "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
    "    next_direction = direction[a]  # 행동 a의 방향\n",
    "\n",
    "    # 행동으로 다음 상태를 결정\n",
    "    if next_direction == \"up\":\n",
    "        s_next = s - 3  # 위로 이동하면 상태값이 3 줄어든다\n",
    "    elif next_direction == \"right\":\n",
    "        s_next = s + 1  # 오른쪽으로 이동하면 상태값이 1 늘어난다\n",
    "    elif next_direction == \"down\":\n",
    "        s_next = s + 3  # 아래로 이동하면 상태값이 3 늘어난다\n",
    "    elif next_direction == \"left\":\n",
    "        s_next = s - 1  # 왼쪽으로 이동하면 상태값이 1 줄어든다\n",
    "\n",
    "    return s_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa 알고리즘으로 행동가치 함수 Q를 수정\n",
    "\n",
    "def Sarsa(s, a, r, s_next, a_next, Q, eta, gamma):\n",
    "\n",
    "    if s_next == 8:  # 목표 지점에 도달한 경우\n",
    "        Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n",
    "\n",
    "    else:\n",
    "        Q[s, a] = Q[s, a] + eta * (r + gamma * Q[s_next, a_next] - Q[s, a])\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa 알고리즘으로 미로를 빠져나오는 함수, 상태 및 행동 그리고 Q값의 히스토리를 출력한다\n",
    "\n",
    "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi):\n",
    "    s = 0  # 시작 지점\n",
    "    a = a_next = get_action(s, Q, epsilon, pi)  # 첫 번째 행동\n",
    "    s_a_history = [[0, np.nan]]  # 에이전트의 행동 및 상태의 히스토리를 기록하는 리스트\n",
    "\n",
    "    while (1):  # 목표 지점에 이를 때까지 반복\n",
    "        a = a_next  # 행동 결정\n",
    "\n",
    "        s_a_history[-1][1] = a\n",
    "        # 현재 상태(마지막이므로 인덱스가 -1)을 히스토리에 추가\n",
    "\n",
    "        s_next = get_s_next(s, a, Q, epsilon, pi)\n",
    "        # 다음 단계의 상태를 구함\n",
    "\n",
    "        s_a_history.append([s_next, np.nan])\n",
    "        # 다음 상태를 히스토리에 추가, 행동은 아직 알 수 없으므로 nan으로 둔다\n",
    "\n",
    "        # 보상을 부여하고 다음 행동을 계산함\n",
    "        if s_next == 8:\n",
    "            r = 1  # 목표 지점에 도달했다면 보상을 부여\n",
    "            a_next = np.nan\n",
    "        else:\n",
    "            r = 0\n",
    "            a_next = get_action(s_next, Q, epsilon, pi)\n",
    "            # 다음 행동 a_next를 계산\n",
    "\n",
    "        # 가치함수를 수정\n",
    "        Q = Sarsa(s, a, r, s_next, a_next, Q, eta, gamma)\n",
    "\n",
    "        # 종료 여부 판정\n",
    "        if s_next == 8:  # 목표 지점에 도달하면 종료\n",
    "            break\n",
    "        else:\n",
    "            s = s_next\n",
    "\n",
    "    return [s_a_history, Q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에피소드: 1\n",
      "1.6982773607833002\n",
      "목표 지점에 이르기까지 걸린 단계 수는 322단계입니다\n",
      "에피소드: 2\n",
      "0.26477084434002573\n",
      "목표 지점에 이르기까지 걸린 단계 수는 48단계입니다\n",
      "에피소드: 3\n",
      "0.09705223345098118\n",
      "목표 지점에 이르기까지 걸린 단계 수는 20단계입니다\n",
      "에피소드: 4\n",
      "0.05228035170147244\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 5\n",
      "0.06509976855187799\n",
      "목표 지점에 이르기까지 걸린 단계 수는 8단계입니다\n",
      "에피소드: 6\n",
      "0.04854552357995945\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 7\n",
      "0.04710717158013372\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 8\n",
      "0.04622830773857006\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 9\n",
      "0.04511878065903757\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 10\n",
      "0.043808197103548\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 11\n",
      "0.04232696566179439\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 12\n",
      "0.04120006857257208\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 13\n",
      "0.04076579925799684\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 14\n",
      "0.04027741913783095\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 15\n",
      "0.03973412076388022\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 16\n",
      "0.039136283385964754\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 17\n",
      "0.03848530658975846\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 18\n",
      "0.037783447918485924\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 19\n",
      "0.037033668200614156\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 20\n",
      "0.03623948712457353\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 21\n",
      "0.03540485067024507\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 22\n",
      "0.034534011281126376\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 23\n",
      "0.033631421103497705\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 24\n",
      "0.03270163819831945\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 25\n",
      "0.031749245321967934\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 26\n",
      "0.030778780651504134\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 27\n",
      "0.02979467968094579\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 28\n",
      "0.02880122742203528\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 29\n",
      "0.02780251999397404\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 30\n",
      "0.0268024346714007\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 31\n",
      "0.02580460747031499\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 32\n",
      "0.02481241738095774\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 33\n",
      "0.02382897639947501\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 34\n",
      "0.022857124562187914\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 35\n",
      "0.021899429244027102\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 36\n",
      "0.020958188043486925\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 37\n",
      "0.020035434638225658\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 38\n",
      "0.019132947056597738\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 39\n",
      "0.018252257869776356\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 40\n",
      "0.017394665865864467\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 41\n",
      "0.016561248820898156\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 42\n",
      "0.015752877031565804\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 43\n",
      "0.014970227320572849\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 44\n",
      "0.014213797267797257\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 45\n",
      "0.013483919458741167\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 46\n",
      "0.012780775576353598\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 47\n",
      "0.012104410193259363\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 48\n",
      "0.011454744148943097\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 49\n",
      "0.010831587420721545\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 50\n",
      "0.010234651418633\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 51\n",
      "0.009663560652892311\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 52\n",
      "0.009117863738558607\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 53\n",
      "0.008597043715752983\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 54\n",
      "0.008100527675384384\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 55\n",
      "0.00762769569009869\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 56\n",
      "0.007177889058254894\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 57\n",
      "0.006750417875352799\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 58\n",
      "0.0063445679526595855\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 59\n",
      "0.005959607106957643\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 60\n",
      "0.0055947908485293185\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 61\n",
      "0.005249367496827917\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 62\n",
      "0.00492258275487667\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 63\n",
      "0.004613683774408739\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 64\n",
      "0.004321922744200957\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 65\n",
      "0.004046560034048796\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 66\n",
      "0.0037868669264599086\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 67\n",
      "0.003542127967479436\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 68\n",
      "0.0033116429671464775\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 69\n",
      "0.0030947286789948514\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 70\n",
      "0.002890720186769724\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 71\n",
      "0.0026989720251916482\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 72\n",
      "0.0025188590601931127\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 73\n",
      "0.0023497771525958777\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 74\n",
      "0.002191143627726211\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 75\n",
      "0.002042397571992871\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 76\n",
      "0.0019029999760015182\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 77\n",
      "0.001772433742352697\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 78\n",
      "0.001650203574881104\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 79\n",
      "0.0015358357647746734\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 80\n",
      "0.0014288778877120656\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 81\n",
      "0.0013288984249544278\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 82\n",
      "0.0012354863201584632\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 83\n",
      "0.0011482504825923723\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 84\n",
      "0.0010668192464008408\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 85\n",
      "0.0009908397946137848\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 86\n",
      "0.0009199775556851808\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 87\n",
      "0.0008539155795195263\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 88\n",
      "0.0007923538991703172\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 89\n",
      "0.0007350088836806101\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 90\n",
      "0.0006816125868758238\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 91\n",
      "0.0006319120963232949\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 92\n",
      "0.0005856688861144432\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 93\n",
      "0.0005426581766277971\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 94\n",
      "0.0005026683039732749\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 95\n",
      "0.00046550010140489206\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 96\n",
      "0.00043096629461436553\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 97\n",
      "0.0003988909124842399\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 98\n",
      "0.0003691087145778482\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 99\n",
      "0.00034146463636985924\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n",
      "에피소드: 100\n",
      "0.00031581325299157026\n",
      "목표 지점에 이르기까지 걸린 단계 수는 4단계입니다\n"
     ]
    }
   ],
   "source": [
    "# Sarsa 알고리즘으로 미로 빠져나오기\n",
    "\n",
    "eta = 0.1  # 학습률\n",
    "gamma = 0.9  # 시간할인율\n",
    "epsilon = 0.5  # ε-greedy 알고리즘 epsilon 초깃값\n",
    "v = np.nanmax(Q, axis=1)  # 각 상태마다 가치의 최댓값을 계산\n",
    "is_continue = True\n",
    "episode = 1\n",
    "\n",
    "while is_continue:  # is_continue의 값이 False가 될 때까지 반복\n",
    "    print(\"에피소드: \" + str(episode))\n",
    "\n",
    "    # ε 값을 조금씩 감소시킴\n",
    "    epsilon = epsilon / 2\n",
    "\n",
    "    # Sarsa 알고리즘으로 미로를 빠져나온 후, 결과로 나온 행동 히스토리와 Q값을 변수에 저장\n",
    "    [s_a_history, Q] = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0)\n",
    "\n",
    "    # 상태가치의 변화\n",
    "    new_v = np.nanmax(Q, axis=1)  # 각 상태마다 행동가치의 최댓값을 계산\n",
    "    print(np.sum(np.abs(new_v - v)))  # 상태가치 함수의 변화를 출력\n",
    "    v = new_v\n",
    "\n",
    "    print(\"목표 지점에 이르기까지 걸린 단계 수는 \" + str(len(s_a_history) - 1) + \"단계입니다\")\n",
    "\n",
    "    # 100 에피소드 반복\n",
    "    episode = episode + 1\n",
    "    if episode > 100:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
